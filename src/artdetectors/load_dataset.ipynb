{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25635b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from pathlib import Path\n",
    "import webdataset as wds\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4779080",
   "metadata": {},
   "source": [
    "## DALLE-E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aebfecf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gahyunyoon/Desktop/classes/eecs6893-final-project/eecs6893-final-proj-venv/lib/python3.13/site-packages/webdataset/compat.py:379: UserWarning: WebDataset(shardshuffle=...) is None; set explicitly to False or a number\n",
      "  warnings.warn(\"WebDataset(shardshuffle=...) is None; set explicitly to False or a number\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading images...\n",
      "Downloaded 100/1000\n",
      "Downloaded 200/1000\n",
      "Downloaded 300/1000\n",
      "Downloaded 400/1000\n",
      "Downloaded 500/1000\n",
      "Downloaded 600/1000\n",
      "Downloaded 700/1000\n",
      "Downloaded 800/1000\n",
      "Downloaded 900/1000\n",
      "Downloaded 1000/1000\n",
      "Done! Downloaded 1000 images (700 train, 300 val, skipped 0)\n"
     ]
    }
   ],
   "source": [
    "# Create directories \n",
    "Path(\"data/train/dalle3\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"data/val/dalle3\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# https://huggingface.co/datasets/CaptionEmporium/dalle3-llama3.2-11b\n",
    "WDS_URLS = \"https://huggingface.co/datasets/CaptionEmporium/dalle3-llama3.2-11b/resolve/main/data/wds/{000000..000137}.tar\"\n",
    "ds = wds.WebDataset(WDS_URLS)\n",
    "\n",
    "print(\"Downloading images...\")\n",
    "count = 0\n",
    "skipped = 0\n",
    "\n",
    "for row in ds:\n",
    "    if count >= 1000:\n",
    "        break\n",
    "    \n",
    "    try:\n",
    "        image_pil = Image.open(io.BytesIO(row[\"jxl\"]))\n",
    "        \n",
    "        # Determine folder\n",
    "        folder = \"train\" if count < 700 else \"val\"\n",
    "        \n",
    "        # Save as JPG\n",
    "        image_pil.convert('RGB').save(f\"data/{folder}/dalle3/img_{count:04d}.jpg\")\n",
    "        \n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print(f\"Downloaded {count}/1000\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "print(f\"Done! Downloaded {count} images (700 train, 300 val, skipped {skipped})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55df69c1",
   "metadata": {},
   "source": [
    "## Midjourney"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d147d9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading images...\n",
      "Downloaded 100/1000\n",
      "Downloaded 200/1000\n",
      "Downloaded 300/1000\n",
      "Downloaded 400/1000\n",
      "Downloaded 500/1000\n",
      "Downloaded 600/1000\n",
      "Downloaded 700/1000\n",
      "Downloaded 800/1000\n",
      "Downloaded 900/1000\n",
      "Downloaded 1000/1000\n",
      "Done! 1000 images\n"
     ]
    }
   ],
   "source": [
    "# Create directories\n",
    "Path(\"data/train/midjourney\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"data/val/midjourney\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load parquet directly (faster, more stable than streaming)\n",
    "# https://huggingface.co/datasets/ava-space/MidJourney\n",
    "df = pd.read_parquet(\"train-00000-of-00002.parquet\")\n",
    "\n",
    "print(\"Downloading images...\")\n",
    "count = 0\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    if count >= 1000:\n",
    "        break\n",
    "    \n",
    "    try:\n",
    "        # Extract bytes and convert to PIL Image\n",
    "        img_bytes = row['image']['bytes']\n",
    "        img = Image.open(io.BytesIO(img_bytes))\n",
    "        \n",
    "        folder = \"train\" if count < 700 else \"val\"\n",
    "        img.convert('RGB').save(f\"data/{folder}/midjourney/img_{count:04d}.jpg\")\n",
    "        \n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print(f\"Downloaded {count}/1000\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "print(f\"Done! {count} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d2b12e",
   "metadata": {},
   "source": [
    "## Authentic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0f7823",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Not enough disk space. Needed: 66.20 GiB (download: 31.42 GiB, generated: 34.79 GiB, post-processed: Unknown size)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m Path(\u001b[33m\"\u001b[39m\u001b[33mdata/val/authentic\u001b[39m\u001b[33m\"\u001b[39m).mkdir(parents=\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m ds = \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhuggan/wikiart\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Group images by genre\u001b[39;00m\n\u001b[32m     14\u001b[39m genre_images = defaultdict(\u001b[38;5;28mlist\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/classes/eecs6893-final-project/eecs6893-final-proj-venv/lib/python3.13/site-packages/datasets/load.py:1417\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001b[39m\n\u001b[32m   1414\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance.as_streaming_dataset(split=split)\n\u001b[32m   1416\u001b[39m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1417\u001b[39m \u001b[43mbuilder_instance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1418\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1419\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1420\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1421\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1422\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1423\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1425\u001b[39m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[32m   1426\u001b[39m keep_in_memory = (\n\u001b[32m   1427\u001b[39m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance.info.dataset_size)\n\u001b[32m   1428\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/classes/eecs6893-final-project/eecs6893-final-proj-venv/lib/python3.13/site-packages/datasets/builder.py:848\u001b[39m, in \u001b[36mDatasetBuilder.download_and_prepare\u001b[39m\u001b[34m(self, output_dir, download_config, download_mode, verification_mode, dl_manager, base_path, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[39m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_local:  \u001b[38;5;66;03m# if cache dir is local, check for available space\u001b[39;00m\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_sufficient_disk_space(\n\u001b[32m    846\u001b[39m         \u001b[38;5;28mself\u001b[39m.info.size_in_bytes \u001b[38;5;129;01mor\u001b[39;00m \u001b[32m0\u001b[39m, directory=Path(\u001b[38;5;28mself\u001b[39m._output_dir).parent\n\u001b[32m    847\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m848\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m    849\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNot enough disk space. Needed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize_str(\u001b[38;5;28mself\u001b[39m.info.size_in_bytes\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[32m0\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (download: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize_str(\u001b[38;5;28mself\u001b[39m.info.download_size\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[32m0\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, generated: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize_str(\u001b[38;5;28mself\u001b[39m.info.dataset_size\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[32m0\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, post-processed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize_str(\u001b[38;5;28mself\u001b[39m.info.post_processing_size\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[32m0\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    850\u001b[39m         )\n\u001b[32m    852\u001b[39m \u001b[38;5;129m@contextlib\u001b[39m.contextmanager\n\u001b[32m    853\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mincomplete_dir\u001b[39m(dirname):\n\u001b[32m    854\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Create temporary dir for dirname and rename on exit.\"\"\"\u001b[39;00m\n",
      "\u001b[31mOSError\u001b[39m: Not enough disk space. Needed: 66.20 GiB (download: 31.42 GiB, generated: 34.79 GiB, post-processed: Unknown size)"
     ]
    }
   ],
   "source": [
    "# Create directories\n",
    "Path(\"data/train/authentic\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"data/val/authentic\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load dataset\n",
    "ds = load_dataset(\"huggan/wikiart\", split=\"train\")\n",
    "\n",
    "# Get genres directly from the dataset\n",
    "all_genres = ds['genre']\n",
    "genre_counts = Counter(all_genres)\n",
    "unique_genres = list(genre_counts.keys())\n",
    "\n",
    "print(f\"Found {len(unique_genres)} genres\")\n",
    "\n",
    "# Sample evenly\n",
    "images_per_genre = 1000 // len(unique_genres)\n",
    "sampled_indices = []\n",
    "\n",
    "for genre in unique_genres:\n",
    "    # Get indices for this genre\n",
    "    genre_indices = [i for i, g in enumerate(all_genres) if g == genre]\n",
    "\n",
    "    # Sample\n",
    "    n_samples = min(images_per_genre, len(genre_indices))\n",
    "    sampled = random.sample(genre_indices, n_samples)\n",
    "    sampled_indices.extend(sampled)\n",
    "\n",
    "# Shuffle and limit to 1000\n",
    "random.shuffle(sampled_indices)\n",
    "sampled_indices = sampled_indices[:1000]\n",
    "\n",
    "# Split 70/30 train/val\n",
    "train_indices = sampled_indices[:700]\n",
    "val_indices = sampled_indices[700:1000]\n",
    "\n",
    "print(\"Saving images...\")\n",
    "\n",
    "# Save train images\n",
    "for i, idx in enumerate(train_indices):\n",
    "    img = ds[idx]['image']\n",
    "    img.convert('RGB').save(f\"data/train/authentic/img_{i:04d}.jpg\")\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"Train: {i + 1}/700\")\n",
    "\n",
    "# Save val images\n",
    "for i, idx in enumerate(val_indices):\n",
    "    img = ds[idx]['image']\n",
    "    img.convert('RGB').save(f\"data/val/authentic/img_{i:04d}.jpg\")\n",
    "    if (i + 1) % 50 == 0:\n",
    "        print(f\"Val: {i + 1}/300\")\n",
    "\n",
    "print(\"Done! Genre distribution maintained across train/val splits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b87a47f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eecs6893-final-proj-venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
